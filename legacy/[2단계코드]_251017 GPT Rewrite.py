# translate_with_gpt_after_deepl.py â€“ 2025-09-08 rev-no-qa-tags + all-corpus-monthly + blank-fix
# ------------------------------------------------------------------
# â‘  â€¦\ë°ì´í„°ìƒì„± ì—ì„œ ìµœì‹  YYYYMMDD_policysearching.xlsx ìë™ ë¡œë“œ
# â‘¡ GPT-ë¦¬ë¼ì´íŒ… â†’ title_ko / content_ko 2 ì»¬ëŸ¼ ì €ì¥  â† (Q&A/Tags ì œê±°)
# â‘¢ Sheet2(â€œMonthlyâ€)ì— content_ko ì „ì²´ ê¸°ë°˜ 'ìµœëŒ€ 10ë¬¸ì¥' ì¢…í•© ìš”ì•½ ì €ì¥
# â‘£ Articles ì‹œíŠ¸(ê¸°ì¡´ ì—´ + 2ê°œ ì—´) + Monthly ì‹œíŠ¸ ì €ì¥ (ì¤„ë°”ê¿ˆ/ì—´ë„ˆë¹„ ì ìš©)
# â‘¤ (ì¶”ê°€) ë‚ ì§œ ê¸°ì¤€ ìµœê·¼ Nì¼ ê¸°ì‚¬ë§Œ ë³„ë„ ì‹œíŠ¸ ì €ì¥ + ì¢…í•© ìš”ì•½ì€ í•´ë‹¹ ì‹œíŠ¸ë§Œ ì°¸ì¡°
# ------------------------------------------------------------------
import os, glob, time, textwrap, re, calendar
from datetime import datetime
import pandas as pd
from dotenv import load_dotenv
from openai import OpenAI
from openpyxl.styles import Alignment  # Excel ê°€ë…ì„±(ì¤„ë°”ê¿ˆ)
from openpyxl.utils import get_column_letter

# ====== ì„¤ì •: ìµœê·¼ Nì¼ ======
RECENT_DAYS = 45  # â† í•„ìš” ì‹œ 30ìœ¼ë¡œ ë³€ê²½
RECENT_SHEET_NAME = f"Articles_Recent{RECENT_DAYS}d"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê²½ë¡œ
SRC_DIR = r"C:\Users\cpryul68\OneDrive\ì „íŒŒì •ì±…(GPT)\ë°ì´í„°ìƒì„±"
GPT_DIR = r"C:\Users\cpryul68\OneDrive\ì „íŒŒì •ì±…(GPT)\ë°ì´í„°ê°€ê³µ(GPT)"
os.makedirs(GPT_DIR, exist_ok=True)

def latest_policy_file(folder: str) -> str | None:
    files = glob.glob(os.path.join(folder, "????????_policysearching.xlsx"))
    return max(files, key=os.path.getmtime) if files else None

INPUT_FILE = latest_policy_file(SRC_DIR)
if not INPUT_FILE:
    raise FileNotFoundError("âŒ ìµœê·¼ *_policysearching.xlsx íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")

stamp        = datetime.now().strftime("%Y%m%d_%H%M%S")
OUTPUT_FILE  = os.path.join(GPT_DIR, f"{stamp}_rewritten.xlsx")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ OpenAI
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ëª¨ë¸ ì„¤ì •(í•„ìš” ì‹œ ì—¬ê¸°ë§Œ ë°”ê¾¸ë©´ ë¨)
MODEL_REWRITE = "gpt-4o"       # ê¸°ì‚¬ ë¦¬ë¼ì´íŒ…
MODEL_SUMMARY = "gpt-5"        # ì¢…í•© ìš”ì•½(5 ì‚¬ìš© ë¶ˆì•ˆì • ì‹œ ìë™ í´ë°±)
MODEL_SUMMARY_FALLBACK = "gpt-4.1-mini"  # ë¹ˆì‘ë‹µ ì‹œ 1íšŒ ì¬ì‹œë„

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Chat Completions í˜¸í™˜ ë˜í¼
_NEW_PARAM_MODELS = re.compile(r"^(gpt-5|o3|o4|gpt-4\.1)")
def chat_create(*, model: str, messages: list, max_tokens: int, temperature: float | None = None):
    kwargs = {"model": model, "messages": messages}
    if _NEW_PARAM_MODELS.match(model):
        kwargs["max_completion_tokens"] = max_tokens
    else:
        kwargs["max_tokens"] = max_tokens
        if temperature is not None:
            kwargs["temperature"] = temperature
    return client.chat.completions.create(**kwargs)

def extract_text(resp) -> str:
    """ë¹ˆ ì‘ë‹µ ë°©ì§€ìš© ì•ˆì „ ì¶”ì¶œê¸°."""
    try:
        txt = resp.choices[0].message.content
        if txt is None:
            return ""
        return str(txt)
    except Exception:
        return ""

def clean_text(s: str) -> str:
    # ë³´ì´ëŠ” ê³µë°±ìœ¼ë¡œë§Œ êµ¬ì„±ëœ ê²½ìš° ì—‘ì…€ì—ì„œ ë¹ˆì¹¸ì²˜ëŸ¼ ë³´ì„ â†’ ì •ê·œí™”
    s = (s or "").replace("\u200b", "").replace("\u2060","").strip()
    # í•œ ë¬¸ë‹¨ ìš”êµ¬ â†’ ì¤„ë°”ê¿ˆì€ ê³µë°±ìœ¼ë¡œ í†µì¼
    s = re.sub(r"\s+", " ", s)
    return s

# ====================== (PROMPT: REWRITE â€“ ê¸°ì¡´ ìœ ì§€) ======================
SYSTEM_PROMPT = textwrap.dedent("""
You are a senior Korean journalist specializing in radioâ€‘spectrum policy and wireless regulation. Your job is to rewrite English news into Korean that is easy for a middleâ€‘school reader to follow without losing policy accuracy.

OUTPUT LANGUAGE: Korean.

STYLE & STRUCTURE
 â€¢ Write the body as one paragraph in this order: Background â†’ Key point â†’ Purpose/implications (6â€“8 sentences).
 â€¢ Use clear, plain Korean (avoid literal translation); keep causeâ†’effect logic; compress redundant phrases.
 â€¢ Maintain a neutral newsroom tone (no marketing, no hype, no speculation).

POLICY ORIENTATION
 â€¢ Surface: (1) the actor (e.g., FCC, NTIA, Ofcom, ARCEP, BNetzA, ACMA, ì¼ë³¸ ì´ë¬´ì„±(MIC), ê³¼ê¸°ì •í†µë¶€), (2) the policy instrument (allocation/assignment/auction/consultation/rulemaking), (3) the band(s), (4) the timeline, and (5) the intended policy outcome.
 â€¢ Keep distinctions precise: allocation(ë¶„ë°°, ITU), allotment(ë°°ì¹˜), assignment(í• ë‹¹).
 â€¢ When mentioned, name band categories (low/mid/mmWave), licensing model (licensed, unlicensed, lightlyâ€‘licensed), exclusivity vs. sharing (e.g., LSA, CBRS/SAS/GAA), and coexistence/interference conditions. Do not add facts not in the source.

PRESERVATION RULES
 â€¢ Preserve all numbers, units, ranges, and dates exactly as written (e.g., 700 MHz, 3.5 GHz, 24.25â€“27.5 GHz, 2025-09-08).
 â€¢ Keep proper nouns/acronyms; on first use, add the Korean name + English acronym (ì˜ˆ: ë¯¸êµ­ ì—°ë°©í†µì‹ ìœ„ì›íšŒ(FCC)).
 â€¢ Convert relative time terms (â€œtodayâ€, â€œyesterdayâ€) to explicit dates ONLY if the date is present in the article; otherwise keep the relative term.

FORMATTING (MANDATORY)
 â€¢ Output exactly the two blocks below, no markdown, no extra text:
   ã€ˆì œëª©ã€‰: concise Korean news headline reflecting the policy action/band/actor.
   ã€ˆë³¸ë¬¸ã€‰: one paragraph, 6â€“8 sentences.

QUALITY CHECKS
 â€¢ Do not invent content, quotes, or numbers.
 â€¢ Remove Q&A, tags, disclaimers, or boilerplate if present in the input.
 â€¢ Prefer verbs such as â€œë°œí‘œí–ˆë‹¤/ê²€í† í•œë‹¤/ì¶”ì§„í•œë‹¤/ê²½ë§¤ë¥¼ ì‹¤ì‹œí–ˆë‹¤/ë°°ì •í–ˆë‹¤/í˜‘ì˜í•œë‹¤/ê°œì •í•œë‹¤â€.
 â€¢ Additionally, use a standard Korean news-reporting tone (ê¸°ì‚¬ë³´ë„ì²´): write in concise, objective Korean in a journalistic reporting style, and do not use polite/formal sentence endings such as "~í•©ë‹ˆë‹¤", "~ì…ë‹ˆë‹¤", or "~ìˆìŠµë‹ˆë‹¤".

ADDITIONAL GUARDRAILS FOR SPECTRUMâ€‘POLICY FIDELITY (append to the end of the prompt)

1) Sourceâ€‘ofâ€‘truth scope
â€¢ Use only information present in the provided article text. Do not add facts from memory or external sources.
â€¢ If a key detail is missing (e.g., band/range, bandwidth, licensing model, timeline, docket/proceeding ID), don't guess.
â€¢ When the headline and body conflict, follow the body and state the action status precisely (e.g., â€œconsultationâ€, â€œproposalâ€, â€œdecision/statementâ€, â€œauctionâ€, â€œassignmentâ€).

2) Terminology precision (no conflation)
â€¢ Do not conflate allocation (ITUâ€‘level), allotment, and assignment (national licensing).
â€¢ Do not infer or rename sharing/licensing frameworks (e.g., CBRS/SAS/GAA, LSA, licenceâ€‘exempt) unless explicitly stated in the source.
â€¢ Do not normalize bands: preserve frequency ranges exactly as written; do not convert â€œ24.25â€“27.5 GHzâ€ into â€œ26 GHz bandâ€ unless the source uses that label.

3) Numbers & units validation
â€¢ Preserve every number, qualifier, and unit verbatim (including â€œup toâ€, â€œat leastâ€, ranges, and decimals).
â€¢ Never round, merge, or â€œsum upâ€ spectrum amounts unless the source provides that figure.
â€¢ Keep MHz vs. GHz correct; do not switch units, reorder ranges, or infer contiguity.

4) Actor & jurisdiction accuracy
â€¢ Attribute actions to the correct authority; do not swap agencies (e.g., NTIA vs. FCC, Ofcom vs. DCMS).
â€¢ If roles differ (e.g., NTIA coordination; FCC rulemaking), reflect each role as stated.
â€¢ If the actor is unclear, refer generically to â€œthe authorityâ€ rather than inventing a specific body.

5) Forbidden inferences
â€¢ Do not assume technology generation (5G/6G), use cases (FWA/NTN), licence type (exclusive/shared/unlicensed), timeline, coverage, or auction revenue unless stated.
â€¢ Do not treat consultations as final decisions; keep the status conservative unless the source declares a binding outcome.

6) Output integrity checks (before returning)
â€¢ Two blocks only (ã€ˆì œëª©ã€‰, ã€ˆë³¸ë¬¸ã€‰); no extra text.
â€¢ Body = 6â€“8 sentences drawn strictly from the source; if facts are insufficient, prefer the lower bound rather than padding with speculation.
â€¢ Ensure all frequency figures, ranges, and key qualifiers present in the source appear in the output; keep docket/proceeding IDs exactly as written (e.g., â€œWT Docket No. 19â€‘354â€, â€œFCC 24â€‘XXâ€).
â€¢ Use the standard Korean newsâ€‘reporting tone (ê¸°ì‚¬ë³´ë„ì²´) with no polite/formal endings (â€œ~í•©ë‹ˆë‹¤/ì…ë‹ˆë‹¤/ìˆìŠµë‹ˆë‹¤â€ are prohibited).

""").strip()
# ==========================================================================

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ GPT í—¬í¼ (ê¸°ì‚¬ ë¦¬ë¼ì´íŒ…)
def gpt_rewrite(title_en: str, body_en: str) -> str:
    prompt = (
        f"ã€ˆOriginal Titleã€‰\n{title_en.strip()}\n\n"
        f"ã€ˆOriginal Article (English)ã€‰\n{body_en.strip()}\n\n"
        "Rewrite in Korean following the system instructions. Output ONLY the two blocks required."
    )
    rsp = chat_create(
        model=MODEL_REWRITE,
        messages=[{"role":"system","content":SYSTEM_PROMPT},
                  {"role":"user","content":prompt}],
        max_tokens=700,
        temperature=0.2
    )
    return extract_text(rsp).strip()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë°ì´í„° ë¡œë“œ
def load_df(path: str) -> pd.DataFrame:
    xl = pd.ExcelFile(path)
    sheet = "Articles" if "Articles" in xl.sheet_names else xl.sheet_names[0]
    df = pd.read_excel(xl, sheet_name=sheet)
    if "date" in df.columns:
        df["date"] = pd.to_datetime(df["date"], errors="coerce")
    else:
        df["date"] = pd.NaT
    return df

df = load_df(INPUT_FILE)
print(f"ğŸ“° {len(df)}ê°œ ê¸°ì‚¬ ë¡œë”© â€“ GPT ë³€í™˜ ì‹œì‘")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ GPT í˜¸ì¶œ
blocks = []
for i, r in df.iterrows():
    try:
        blk = gpt_rewrite(str(r.get("title", "")), str(r.get("content", "")))
    except Exception as e:
        blk = f"[GPT Error] {e}"
    blocks.append(blk)
    print(f" Â· {i+1}/{len(df)} ì™„ë£Œ")
    time.sleep(1)  # API rate-limit ì™„ì¶©

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë¸”ë¡ íŒŒì‹± (Q&A/Tags ì œê±° ë²„ì „)
def parse_block(txt: str):
    if txt.startswith("[GPT Error]"):
        return ("[Error]", txt)
    t = textwrap.dedent(txt).strip()
    title = re.search(r"ã€ˆì œëª©ã€‰[:ï¼š]\s*(.+)", t)
    body  = re.search(r"ã€ˆë³¸ë¬¸ã€‰[:ï¼š]\s*(.+)", t, re.S)
    return (
        title.group(1).strip() if title else "",
        re.sub(r"\s+"," ", body.group(1).strip()) if body else ""
    )

parsed = [parse_block(b) for b in blocks]
df[["title_ko","content_ko"]] = pd.DataFrame(parsed, index=df.index)
df.drop(columns=["question_ko","tags"], errors="ignore", inplace=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ (ì‹ ê·œ) ìµœê·¼ Nì¼ ê¸°ì‚¬ í•„í„°ë§
def filter_recent_articles(df_in: pd.DataFrame, days: int) -> pd.DataFrame:
    """date ì—´ì„ ê¸°ì¤€ìœ¼ë¡œ ìµœê·¼ Nì¼ ì´ë‚´ ê¸°ì‚¬ë§Œ ë°˜í™˜(ë‚ ì§œ NaTëŠ” ì œì™¸)."""
    if "date" not in df_in.columns:
        return df_in.iloc[0:0].copy()
    df_work = df_in.copy()
    if not pd.api.types.is_datetime64_any_dtype(df_work["date"]):
        df_work["date"] = pd.to_datetime(df_work["date"], errors="coerce")
    # ì˜¤ëŠ˜(ë¡œì»¬) ê¸°ì¤€ Nì¼
    now = pd.Timestamp.now()
    cutoff = now.normalize() - pd.Timedelta(days=days)
    mask = df_work["date"].notna() & (df_work["date"] >= cutoff)
    return df_work.loc[mask].copy()

recent_df = filter_recent_articles(df, RECENT_DAYS)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì¢…í•© ìš”ì•½ (ìµœê·¼ Nì¼ ê¸°ì‚¬ë§Œ)
def policy_summary_all(texts_ko: list[str]) -> str:
    corpus = [t for t in texts_ko if isinstance(t, str) and t.strip()]
    joined = "\n\n".join(corpus)[:10000]  # í† í° ì•ˆì „ì¥ì¹˜
    if not joined:
        return "ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
    prompt = textwrap.dedent(f"""
    You are preparing a **monthly spectrum policy digest** for decisionâ€‘makers.

    INPUT
    â€¢ Below is a concatenated set of Korean article bodies about spectrum/wireless policy (rewritten outputs).
    â€¢ Articles may cover multiple countries/regions and regulators.

    GOAL
    â€¢ For each country/region detected in the input, **select the 1â€“2 most consequential items** from a spectrumâ€‘policy perspective, then compress them into **one sentence per country**.
    â€¢ Consequential = nationwide allocation/assignment/auction; major consultation/rulemaking; new or expanded sharing frameworks (e.g., CBRS/SAS/GAA, LSA); crossâ€‘border coordination; 6G candidate bands; opening/repurposing â‰¥100 MHz; actions impacting 6 GHz, 3.x GHz, 24â€“29 GHz, or comparable strategic bands.
    â€¢ If more than 10 countries appear, keep **the top 8â€“10** by impact/scope and omit the rest.

    COUNTRY/REGULATOR DETECTION (nonâ€‘exhaustive examples)
    â€¢ U.S. (ë¯¸êµ­ ì—°ë°©í†µì‹ ìœ„ì›íšŒ(FCC), NTIA), EU/CEPT/ECC, UK(Ofcom), France(ARCEP), Germany(BNetzA), Italy(AGCOM),
      Spain(CNMC), Sweden(PTS), Finland(Traficom), Netherlands(ACM), Canada(ISED), Australia(ACMA),
      Japan(ì´ë¬´ì„±(MIC)), South Korea(ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€(MSIT)), China(MIIT), India(DoT), Singapore(IMDA), etc.
    â€¢ Infer country from explicit mentions of regulators/governments/agencies and context within the articles.

    WRITING RULES (OUTPUT LANGUAGE: Korean)
    â€¢ Produce **ONE paragraph total**, **â‰¤ 10 sentences**.
    â€¢ Write **exactly one sentence per selected country**, merging that countryâ€™s 1â€“2 items via commas/semicolons.
    â€¢ Start each sentence with **êµ­ê°€ëª…(ì£¼ìš” ê·œì œê¸°ê´€)**, then state: actor â†’ band(s) â†’ policy instrument (allocation/assignment/auction/consultation/rulemaking/sharing) â†’ timeline â†’ intended outcome/implications.
    â€¢ Neutral newsroom tone; strictly factâ€‘based; no speculation or opinions.
    â€¢ Preserve all numbers/units/dates/ranges exactly as given (e.g., MHz, GHz, 24.25â€“27.5 GHz, 2025-09-08).
    â€¢ On first mention of a regulator, you may include the Korean full name + English acronym (ì˜ˆ: ë¯¸êµ­ ì—°ë°©í†µì‹ ìœ„ì›íšŒ(FCC)).
    â€¢ Sanitize input: ignore Q&A sections, tags, disclaimers, unrelated device/market commentary.

    IMPORTANT
    â€¢ Do **not** invent facts, quotes, or figures.
    â€¢ If multiple items exist for one country, **combine them into a single compact sentence**.
    â€¢ If content is sparse for a country, include it only if it materially changes spectrum availability or policy direction.

    OUTPUT
    â€¢ Return a **single Korean paragraph** (no title or bullets), up to 10 sentences, each sentence covering one country as specified.
    â€¢ Additionally, when producing the final summary paragraph, order the sentences by country in the following sequence: South Korea â†’ United States â†’ United Kingdom â†’ Japan â†’ all other countries (in any order after these four).
    
    ARTICLES (Korean, aggregated):
    {joined}
    """)
    rsp = chat_create(
        model=MODEL_SUMMARY,
        messages=[{"role":"system","content":"You are a Korean journalist specializing in radio-spectrum policy and wireless regulation. Output Korean."},
                  {"role":"user","content":prompt}],
        max_tokens=600,
        temperature=None
    )
    raw = extract_text(rsp)
    if not raw or not raw.strip():
        rsp2 = chat_create(
            model=MODEL_SUMMARY_FALLBACK,
            messages=[{"role":"system","content":"You are a Korean journalist specializing in radio-spectrum policy and wireless regulation. Output Korean."},
                      {"role":"user","content":prompt}],
            max_tokens=600,
            temperature=0.2
        )
        raw = extract_text(rsp2)
    cleaned = clean_text(raw)
    return cleaned if cleaned else "ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

# ===== ìš”ì•½ì€ 'ìµœê·¼ Nì¼' ê¸°ì‚¬ë§Œ ì°¸ì¡° =====
recent_texts_ko = recent_df["content_ko"].tolist() if "content_ko" in recent_df.columns else []
monthly_para  = policy_summary_all(recent_texts_ko)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì €ì¥
with pd.ExcelWriter(OUTPUT_FILE, engine="openpyxl") as wr:
    # 1) Articles (ì „ì²´)
    #   - ì „ì²´ ì‹œíŠ¸ ì €ì¥ ì „: ë‚ ì§œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
    df_all_out = df.copy()
    if "date" in df_all_out.columns and pd.api.types.is_datetime64_any_dtype(df_all_out["date"]):
        df_all_out["date"] = df_all_out["date"].dt.strftime("%Y-%m-%d")
    df_all_out.to_excel(wr, sheet_name="Articles", index=False)

    # 2) (ì‹ ê·œ) ìµœê·¼ Nì¼ ì‹œíŠ¸
    recent_out = recent_df.copy()
    if "date" in recent_out.columns and pd.api.types.is_datetime64_any_dtype(recent_out["date"]):
        recent_out["date"] = recent_out["date"].dt.strftime("%Y-%m-%d")
    recent_out.to_excel(wr, sheet_name=RECENT_SHEET_NAME, index=False)

    # 3) Monthly (ìš”ì•½ = ìµœê·¼ Nì¼ ê¸°ì‚¬ë§Œ ê¸°ë°˜)
    month_name = calendar.month_name[int(datetime.now().strftime('%m'))]
    title = f"{datetime.now().year}ë…„ {datetime.now().month}ì›” ìµœê·¼ ì£¼íŒŒìˆ˜ ì •ì±… ì¢…í•© ìš”ì•½"
    monthly_df = pd.DataFrame({"Monthly Summary":[title], "Content":[monthly_para]})
    monthly_df.to_excel(wr, sheet_name="Monthly", index=False)

    # ë³´ê¸° ì¢‹ê²Œ: Monthly ì‹œíŠ¸ ì¤„ë°”ê¿ˆ/ì—´ë„ˆë¹„ ì ìš©
    ws = wr.sheets["Monthly"]
    ws.column_dimensions[get_column_letter(1)].width = 36   # Monthly Summary
    ws.column_dimensions[get_column_letter(2)].width = 120  # Content
    for row in ws.iter_rows(min_row=1, max_row=ws.max_row, min_col=1, max_col=2):
        for cell in row:
            cell.alignment = Alignment(wrap_text=True, vertical="top")

print(f"\nâœ… ë³€í™˜ ì™„ë£Œ â†’ {OUTPUT_FILE}\n"
      f"   Â· ì „ì²´ ê¸°ì‚¬ ì‹œíŠ¸: Articles\n"
      f"   Â· ìµœê·¼ {RECENT_DAYS}ì¼ ì‹œíŠ¸: {RECENT_SHEET_NAME}\n"
      f"   Â· Monthly ìš”ì•½ì€ ìµœê·¼ {RECENT_DAYS}ì¼ ê¸°ì‚¬ë§Œ ë°˜ì˜")
